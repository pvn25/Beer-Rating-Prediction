{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "def readGz(f):\n",
    "    for l in gzip.open(f):\n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 (Ratings prediction) - Task1 comes later\n",
    "### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha =  4.23198\n"
     ]
    }
   ],
   "source": [
    "### Rating baseline: compute averages for each user, or return the global average \n",
    "### if we've never seen the user before\n",
    "\n",
    "allRatings = []\n",
    "userRatings = defaultdict(list)\n",
    "\n",
    "valRatings = []\n",
    "valUserRatings = defaultdict(list)\n",
    "count = 0\n",
    "for l in readGz(\"train.json.gz\"):\n",
    "    user,item = l['reviewerID'],l['itemID']\n",
    "    \n",
    "    if(count < 100000):\n",
    "        allRatings.append(l['rating'])\n",
    "        userRatings[user].append(l['rating'])\n",
    "    elif(count < 200000):\n",
    "        valRatings.append(l['rating'])\n",
    "        valUserRatings[user].append(l['rating'])\n",
    "        \n",
    "    count = count + 1\n",
    "\n",
    "globalAverage = sum(allRatings) / len(allRatings)\n",
    "userAverage = {}\n",
    "for u in userRatings:\n",
    "    userAverage[u] = sum(userRatings[u]) / len(userRatings[u])\n",
    "\n",
    "print \"Alpha = \", globalAverage  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Iu = {}\n",
    "Ui = {}\n",
    "lamda = 6\n",
    "betaU = {}\n",
    "betaI = {}\n",
    "n_train = 0\n",
    "count = 0\n",
    "review = {}\n",
    "for l in readGz(\"train.json.gz\"):\n",
    "    if(count > 100000):\n",
    "        break\n",
    "    else:\n",
    "        count += 1\n",
    "    n_train += 1\n",
    "    user, item, rating, re = l['reviewerID'], l['itemID'], l['rating'], l['reviewText']\n",
    "    if user not in Iu:\n",
    "        Iu[user] = {}\n",
    "        betaU[user] = 0\n",
    "        review[user]= {}\n",
    "    if item not in Ui:\n",
    "        Ui[item] = {}\n",
    "        betaI[item] = 0\n",
    "    Iu[user][item] = rating\n",
    "    Ui[item][user] = rating\n",
    "    \n",
    "    if(classifier.classify(word_feats(re)) == 'pos'):\n",
    "        review[user][item] = 1\n",
    "    else:\n",
    "        review[user][item] = -1\n",
    "\n",
    "alpha = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lamda = 6\n",
    "for i in range(50):\n",
    "    s = 0\n",
    "    for user in Iu.keys():\n",
    "        for item, rating in Iu[user].iteritems():\n",
    "            s +=  (rating - (betaU[user] + betaI[item]))\n",
    "    alpha = s*1.0/n_train\n",
    "    \n",
    "    for user in Iu.keys():\n",
    "        s = 0\n",
    "        for item, rating in Iu[user].iteritems():\n",
    "            s += (rating - (alpha+betaI[item]))\n",
    "        betaU[user] = s*1.0/(lamda+len(Iu[user]))\n",
    "    \n",
    "    \n",
    "    for item in Ui.keys():\n",
    "        s = 0\n",
    "        for user, rating in Ui[item].iteritems():\n",
    "            s += (rating - (alpha+betaU[user]))\n",
    "        betaI[item] =  s*1.0/(lamda+len(Ui[item]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_users = len(Iu.keys())\n",
    "n_items = len(Ui.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fill the indices for user and item\n",
    "userId = {}\n",
    "itemId = {}\n",
    "\n",
    "for user in Iu.keys():\n",
    "    userId[user] = Iu.keys().index(user)\n",
    "\n",
    "for item in Ui.keys():\n",
    "    itemId[item] = Ui.keys().index(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row_indices = []\n",
    "col_indices = []\n",
    "r = []\n",
    "\n",
    "for user in Iu.keys():\n",
    "    for item in Iu[user]:\n",
    "        row_indices.append(userId[user])\n",
    "        col_indices.append(itemId[item])\n",
    "        r.append(Iu[user][item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csc_matrix\n",
    "# csc_matrix((data, (row, col)), shape=(3, 3)).toarray()\n",
    "\n",
    "R = csc_matrix((r, (row_indices, col_indices)), shape = (len(userId.keys()), len(itemId.keys())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Do NMF of the matrix\n",
    "# nmf = NMF(n_components=10, tol=.01)\n",
    "# >>> Xnmf = nmf.fit_transform(X)\n",
    "from sklearn.decomposition import NMF\n",
    "nmf = NMF(n_components = 5)\n",
    "u = nmf.fit_transform(R)\n",
    "v = nmf.components_\n",
    "# print u.reconstruction_err_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on validation set =  1.22645960195  for lamda =  6\n"
     ]
    }
   ],
   "source": [
    "mse = 0\n",
    "count = 0\n",
    "c = 0\n",
    "for l in readGz(\"train.json.gz\"):\n",
    "    count += 1\n",
    "    if(count < 100000):\n",
    "        continue\n",
    "    elif (count > 200000):\n",
    "        break\n",
    "    c += 1\n",
    "    user, item, rating = l['reviewerID'], l['itemID'], l['rating']\n",
    "    val = alpha   \n",
    "    mse += (float(rating) - val)**2\n",
    "    \n",
    "print \"MSE on validation set = \", mse*1.0/c, \" for lamda = \", lamda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6: We now have the required alpha, betaU and betaI values. Let's now find the MSE on validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-337-cb24ce8fb603>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreadGz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train.json.gz\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mcount\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m100000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-d48999ef8b29>\u001b[0m in \u001b[0;36mreadGz\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mreadGz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgzip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[1;32myield\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mse = 0\n",
    "count = 0\n",
    "c = 0\n",
    "for l in readGz(\"train.json.gz\"):\n",
    "    count += 1\n",
    "    if(count < 100000):\n",
    "        continue\n",
    "    elif (count > 200000):\n",
    "        break\n",
    "    c += 1\n",
    "    user, item, rating = l['reviewerID'], l['itemID'], l['rating']\n",
    "    \n",
    "    val = alpha\n",
    "    if user in betaU:\n",
    "        val += betaU[user]\n",
    "    if item in betaI:\n",
    "        val += betaI[item]\n",
    "    \n",
    "    if user in betaU and item in betaI:\n",
    "        val = np.dot(gu[userId[user]], gi[itemId[item]])\n",
    "    else:\n",
    "        val = alpha\n",
    "        if user in betaU:\n",
    "            val += betaU[user]\n",
    "        if item in betaI:\n",
    "            val += betaI[item]\n",
    "        \n",
    "    if val > 5:\n",
    "        val = 5\n",
    "        \n",
    "    mse += (float(rating) - val)**2    \n",
    "    \n",
    "print \"MSE on validation set = \", mse*1.0/c, \" for lamda = \", lamda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review[user][item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0300955374335\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "avgOffset = 0\n",
    "for user in Iu.keys():\n",
    "    for item in Iu[user]:\n",
    "        c += 1\n",
    "        avgOffset += np.dot(gu[userId[user]], gi[itemId[item]])\n",
    "avgOffset /= c\n",
    "print avgOffset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "lamda = 15\n",
    "gu = np.random.rand(len(Iu.keys()), k)\n",
    "gi = np.random.rand(len(Ui.keys()), k)\n",
    "fac = 0\n",
    "eta = 0.001\n",
    "etas = 0.00001\n",
    "for m in range(30):\n",
    "    print m\n",
    "    for user in Iu.keys():\n",
    "        for item in Iu[user]:\n",
    "#             if(user in review and item in review[user]):\n",
    "#                 pref = review[user][item]\n",
    "#             else:\n",
    "#                 pref = 0\n",
    "                \n",
    "            factor = (alpha+betaU[user]+betaI[item]+np.dot(gu[userId[user]], gi[itemId[item]]) - Iu[user][item])\n",
    "#             factor = (np.dot(gu[userId[user]], gi[itemId[item]]) - Iu[user][item])\n",
    "#             factor = (alpha+betaU[user]+betaI[item]+fac*pref - Iu[user][item])\n",
    "            alpha = alpha - etas*2*factor\n",
    "            betaU[user] = betaU[user] - etas*(2*factor + 2*lamda*betaU[user])\n",
    "            betaI[item] = betaI[item] - etas*(2*factor + 2*lamda*betaI[item])\n",
    "#             fac = fac - eta*(2*factor*pref + 2*lamda*fac)\n",
    "\n",
    "            for j in range(0, k):\n",
    "                gu[userId[user]][j] -= eta*(2*factor*gi[itemId[item]][j] + 2*lamda*gu[userId[user]][j])\n",
    "            for j in range(k):\n",
    "                gi[itemId[item]][j] -= eta*(2*factor*gu[userId[user]][j] + 2*lamda*gi[itemId[item]][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.2254494426967755"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User with min beta =  U052814411 \n",
      "Item with min beta =  I071368828\n",
      "User with max beta =  U816486110 \n",
      "Item with max beta =  I558325415\n"
     ]
    }
   ],
   "source": [
    "minUserBeta = None\n",
    "maxUserBeta = None\n",
    "for user in betaU.keys():\n",
    "    if(minUserBeta == None):\n",
    "        minUserBeta = user\n",
    "        maxUserBeta = user\n",
    "    elif (betaU[minUserBeta] > betaU[user]):\n",
    "        minUserBeta = user\n",
    "    if (betaU[maxUserBeta] < betaU[user]):\n",
    "        maxUserBeta = user\n",
    "        \n",
    "minItemBeta = None\n",
    "maxItemBeta = None\n",
    "for item in betaI.keys():\n",
    "    if(minItemBeta == None):\n",
    "        minItemBeta = item\n",
    "        maxItemBeta = item\n",
    "    elif (betaI[minItemBeta] > betaI[item]):\n",
    "        minItemBeta = item\n",
    "    if(betaI[maxItemBeta] < betaI[item]):\n",
    "        maxItemBeta = item\n",
    "\n",
    "print \"User with min beta = \", minUserBeta, \"\\nItem with min beta = \", minItemBeta\n",
    "print \"User with max beta = \", maxUserBeta, \"\\nItem with max beta = \", maxItemBeta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = open('pairs_Rating.txt')\n",
    "data.next()\n",
    "answer = open('myanswer8.csv', 'w')\n",
    "predictions = []\n",
    "for line in data:\n",
    "    user, item = line.strip().split('-')\n",
    "    val = 0\n",
    "#     if(user in betaU and item in betaI):\n",
    "#         val = np.dot(u[userId[user]], v[:, itemId[item]])\n",
    "#     else:\n",
    "    val = alpha\n",
    "    if user in betaU.keys():\n",
    "        val += betaU[user]\n",
    "    if item in betaI.keys():\n",
    "        val += betaI[item]\n",
    "        \n",
    "    if item in betaI and \n",
    "    predictions.append(val)\n",
    "    answer.write(user+'-'+item+','+str(val)+'\\n')\n",
    "\n",
    "answer.close()\n",
    "data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Task 1) Helpfulness prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Helpfulness baseline: similar to the above. \n",
    "### Compute the global average helpfulness rate, and the average helpfulness rate for each user\n",
    "\n",
    "allHelpful = [] \n",
    "userHelpful = defaultdict(list)\n",
    "userHelpful10 = defaultdict(list)\n",
    "\n",
    "valHelpful = []\n",
    "valUserHelpful = defaultdict(list)\n",
    "\n",
    "userRating = defaultdict(list)\n",
    "itemRating = defaultdict(list)\n",
    "\n",
    "itemHelpful = defaultdict(list)\n",
    "earliestReviewTime = {}\n",
    "\n",
    "c = 0\n",
    "price = 0\n",
    "for l in readGz(\"train.json.gz\"):\n",
    "    if 'price' in l:\n",
    "        price += l['price']\n",
    "        c += 1.0\n",
    "avgprice = price/c\n",
    "\n",
    "valcount= 0\n",
    "count = 0\n",
    "for l in readGz(\"train.json.gz\"):\n",
    "    user, item = l['reviewerID'],l['itemID']\n",
    "    outOf = int(l['helpful']['outOf'])    \n",
    "    count = count + 1\n",
    "    \n",
    "    if 'price' in l:\n",
    "        pr = l['price']\n",
    "    else:\n",
    "        pr = avgprice\n",
    "    \n",
    "    if(count <= 100000):\n",
    "        itemRating[item].append(float(l['rating']))\n",
    "        if(outOf >= 20):\n",
    "            allHelpful.append(l['helpful'])\n",
    "            userHelpful[user].append([l['helpful'], l['reviewText'],l['rating'], l['summary'], item, outOf, pr])             \n",
    "        elif(outOf >0  and outOf < 20):\n",
    "            allHelpful.append(l['helpful'])\n",
    "            userHelpful10[user].append([l['helpful'], l['reviewText'],l['rating'], l['summary'], item, outOf, pr])\n",
    "    elif(count <= 200000):\n",
    "        valcount += 1\n",
    "        if(outOf > 0):\n",
    "            valHelpful.append(l['helpful'])\n",
    "            valUserHelpful[user].append([l['helpful'], l['reviewText'], l['rating'], l['summary'], item, outOf, pr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avgRating = 0\n",
    "count = 0\n",
    "for u in userHelpful:\n",
    "    for review in userHelpful[u]:\n",
    "        count += 1\n",
    "        avgRating += review[2]\n",
    "for u in userHelpful10:\n",
    "    for review in userHelpful10[u]:\n",
    "        count += 1\n",
    "        avgRating += review[2]\n",
    "    \n",
    "avgRating /= count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def feat2(x):\n",
    "    \n",
    "    reviewLen = len(x[1].split(\" \"))\n",
    "    rlen = len(x[1])\n",
    "    words = len(x[1].strip().split(\" \"))\n",
    "    sentences = (x[1].strip().split(\".\"))\n",
    "    n_sent = 0\n",
    "    for sentence in sentences:\n",
    "        if len(sentence) > 3:\n",
    "            n_sent += 1\n",
    " \n",
    "    feature = [] \n",
    "\n",
    "    rating = float(x[2]) \n",
    "    feature += [rating] \n",
    "    \n",
    "    if len(itemRating[x[4]]) == 0:\n",
    "        avgr = avgRating\n",
    "    else:        \n",
    "        avgr = np.mean(itemRating[x[4]])\n",
    "        \n",
    "    feature += [abs(avgr-rating)]\n",
    "    \n",
    "    # out\n",
    "    feature += [float(x[5])] \n",
    "    \n",
    "    upperWordCount = 0\n",
    "    wordCount = 0\n",
    "    pattern = re.compile(\"\\w+\")\n",
    "    for word in x[1].split():\n",
    "        if pattern.search(word):\n",
    "            wordCount += 1\n",
    "            if word.isupper():\n",
    "                upperWordCount += 1\n",
    "    \n",
    "    feature += [upperWordCount]\n",
    "    feature += [wordCount]\n",
    "    \n",
    "    feature += countEx(x[1], '!')\n",
    "    \n",
    "    feature += countEx(x[1], '?')\n",
    "    \n",
    "    if(x[1].count('.') > 0):\n",
    "        avgSentLen = rlen*1.0/x[1].count('.')\n",
    "    else:\n",
    "        avgSentLen = 0\n",
    "    \n",
    "    feature += [avgSentLen]\n",
    "    feature += [rlen]\n",
    "    \n",
    "#      # Deviation of rating from average rating - good\n",
    "#     feature += [avgRating-rating]\n",
    "    \n",
    "#     print avgr\n",
    "    \n",
    "#     # Unsigned deviation - good\n",
    "#     feature += [abs(rating-avgRating)] \n",
    "  \n",
    "#     return [1,review['rating'], (review['rating']-avgRating[review['itemID']])**2, \n",
    "#             review['helpful']['outOf'],wordCount, upperWordCount,reviewText.count('!'),\n",
    "#             reviewText.count('?'),avgSentLen, len(reviewText)]\n",
    "    return feature   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I'm denoting beta = nHelpful/outOf\n",
    "import math\n",
    "# def feat(x):\n",
    "import re\n",
    "#     return [1] + [len(x[1].split(\" \"))] + [float(x[2])]\n",
    "\n",
    "def beta(x):\n",
    "    val = (x[0]['nHelpful']*1.0/x[0]['outOf'])\n",
    "    return val\n",
    "    \n",
    "\n",
    "# Construct training and validation datasets\n",
    "trainX = []\n",
    "trainY = []\n",
    "for u in userHelpful.keys():\n",
    "    for review in userHelpful[u]:\n",
    "        trainX.append(feat2(review))\n",
    "        trainY.append(beta(review)) \n",
    "\n",
    "trainXX = []\n",
    "trainXY = []\n",
    "for u in userHelpful10.keys():\n",
    "    for review in userHelpful10[u]:\n",
    "        trainXX.append(feat2(review))\n",
    "        trainXY.append(beta(review))\n",
    "\n",
    "outOfList = []\n",
    "valX = []\n",
    "valY = []\n",
    "for u in valUserHelpful.keys():\n",
    "    for review in valUserHelpful[u]:\n",
    "            outOfList.append(review[0]['outOf'])\n",
    "            valX.append(feat2(review))\n",
    "            valY.append(beta(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "theta1,residuals,rank,s = numpy.linalg.lstsq(np.array(trainX), np.array(trainY))\n",
    "theta2,residuals,rank,s = numpy.linalg.lstsq(np.array(trainXX), np.array(trainXY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False  True False False False False False  True  True]\n",
      "[8 6 1 2 3 4 5 7 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Recursive Feature Elimination\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# create a base classifier used to evaluate a subset of attributes\n",
    "model = RandomForestRegressor()\n",
    "# create the RFE model and select 3 attributes\n",
    "rfe = RFE(model, 3)\n",
    "rfe = rfe.fit(trainX, trainY)\n",
    "# summarize the selection of the attributes\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mae', max_depth=6,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rgr = RandomForestRegressor(n_estimators = 10, criterion = 'mae', max_depth = 5)\n",
    "rgr.fit(trainX, trainY)\n",
    "rgr10 = RandomForestRegressor(n_estimators = 10, criterion = 'mae', max_depth = 6)       \n",
    "rgr10.fit(trainXX, trainXY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE =  0.17431\n"
     ]
    }
   ],
   "source": [
    "prediction = []\n",
    "prediction10 = []\n",
    "# for i in range(len(valX)):\n",
    "#     if valX[i][3] >= 10:\n",
    "#         prediction.append(np.dot(theta1, valX[i]))\n",
    "prediction = (rgr.predict(valX))\n",
    "#     else:\n",
    "#         prediction.append(np.dot(theta2, valX[i]))\n",
    "prediction10 = (rgr10.predict(valX))\n",
    "\n",
    "ans = []\n",
    "true = []\n",
    "for i in range(len(valX)):\n",
    "    if outOfList[i] > 10:\n",
    "        ans.append(np.round(outOfList[i]*prediction[i]))\n",
    "    else:\n",
    "        ans.append(np.round(outOfList[i]*prediction10[i]))\n",
    "    true.append(outOfList[i]*valY[i])\n",
    "    \n",
    "print \"MAE = \", sum([abs(a-b) for (a, b) in zip(ans, true)])*1.0/valcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating test set data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "userItemPairs = []\n",
    "data = open('pairs_Helpful.txt')\n",
    "data.next()\n",
    "\n",
    "def extract(x, outOf):\n",
    "#     print outOf\n",
    "    user, item = l['reviewerID'], l['itemID']\n",
    "    if 'price' in x:\n",
    "        pr = x['price']\n",
    "    else:\n",
    "        pr = avgprice\n",
    "    m = [x['helpful'], x['reviewText'], x['rating'], x['summary'], item, outOf, pr]\n",
    "    \n",
    "    if item not in earliestReviewTime.keys():\n",
    "        earliestReviewTime[item] = int(l['unixReviewTime'])\n",
    "        \n",
    "    if earliestReviewTime[item] > int(l['unixReviewTime']):\n",
    "        earliestReviewTime[item] = int(l['unixReviewTime'])\n",
    "        \n",
    "    return feat2(m)\n",
    "\n",
    "# Generate all user item pairs we care about\n",
    "for line in data:\n",
    "    user, item, outOf = line.strip().split('-')\n",
    "    userItemPairs.append([user, item, outOf])\n",
    "\n",
    "testX = []\n",
    "# testY = []\n",
    "reviews = {}\n",
    "for l in readGz(\"test_Helpful.json.gz\"):\n",
    "    user, item = l['reviewerID'],l['itemID']\n",
    "    if user not in reviews.keys():\n",
    "        reviews[user] = {}\n",
    "    reviews[user][item] = l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Save this as output to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answer = open(\"myanswer4.csv\", 'w')\n",
    "count = 0\n",
    "for (user, item, outOf) in userItemPairs:\n",
    "    count += 1    \n",
    "    feature_vector = extract(reviews[user][item], outOf)\n",
    "#     print len(feature_vector)\n",
    "#     break\n",
    "    if feature_vector[3] >= 10:\n",
    "        p = rgr.predict([feature_vector])[0]\n",
    "    else:\n",
    "        p = rgr10.predict([feature_vector])[0]\n",
    "    v = np.round(float(outOf)*p)\n",
    "    if v > float(outOf):\n",
    "        v = outOf\n",
    "    s = user + \"-\" + item + \"-\" + outOf + ',' + str(v) + \"\\n\"\n",
    "    answer.write(s)\n",
    "answer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle user name - sriramravindran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 1500 instances, test on 500 instances\n",
      "accuracy: 0.728\n",
      "Most Informative Features\n",
      "             magnificent = True              pos : neg    =     15.0 : 1.0\n",
      "             outstanding = True              pos : neg    =     13.6 : 1.0\n",
      "               insulting = True              neg : pos    =     13.0 : 1.0\n",
      "              vulnerable = True              pos : neg    =     12.3 : 1.0\n",
      "               ludicrous = True              neg : pos    =     11.8 : 1.0\n",
      "                  avoids = True              pos : neg    =     11.7 : 1.0\n",
      "             uninvolving = True              neg : pos    =     11.7 : 1.0\n",
      "              astounding = True              pos : neg    =     10.3 : 1.0\n",
      "             fascination = True              pos : neg    =     10.3 : 1.0\n",
      "                 idiotic = True              neg : pos    =      9.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# nltk.download()\n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    " \n",
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')\n",
    " \n",
    "negfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "posfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    " \n",
    "negcutoff = len(negfeats)*3/4\n",
    "poscutoff = len(posfeats)*3/4\n",
    " \n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "print 'train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats))\n",
    " \n",
    "classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "print 'accuracy:', nltk.classify.util.accuracy(classifier, testfeats)\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def countEx(x):\n",
    "#     # 0 - periods\n",
    "#     # 1 - !\n",
    "#     # 2 - -\n",
    "#     # 3 - ;\n",
    "#     # 4 - :\n",
    "#     # 5 - $\n",
    "#     vec = [0]*2\n",
    "    \n",
    "#     mp = {'.':0, '-':2, ';':3, ':':4, ',':5, '\"':6}\n",
    "#     mq = {'!':5, '?':1}\n",
    "#     mn = {'<':2, '>':3, ')':7, '(':8, '*':9, '^':1, '#':2}    \n",
    "    \n",
    "#     for i in x:\n",
    "#         if i in mp.keys():\n",
    "#             vec[0] += 1\n",
    "#         if i in mq.keys():\n",
    "#             vec[1] += 1                \n",
    "            \n",
    "#     return vec \n",
    "def countEx(x, p):\n",
    "    l = len(x.strip().split(\" \"))\n",
    "    c = 0\n",
    "    for i in x:\n",
    "        if i == p:\n",
    "            c += 1\n",
    "            \n",
    "    return [c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numbersIn(x):\n",
    "    count = 0\n",
    "    for word in x.strip().split(\" \"):\n",
    "        try:\n",
    "            float(s)\n",
    "            count += 1\n",
    "        except:\n",
    "            pass\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wordsInAllCaps(x):\n",
    "    words = x.strip().split(\" \")\n",
    "    count = sum([1 for word in words if word.isupper])\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def smileyFraction(review):\n",
    "    smiley1 = [':)',':(',':D',':d',';)',':o',':O',';(',':|',':*',':P',':p',':$',':&',':@','x(','X(','X-',':S',':s',\n",
    "               '8|','B|',':x',':?']\n",
    "    smiley2 = [':=)',':-)',':-)',':=(',':-(',':=D',':-D',':=d','8=)','8-)','B=)','B-)',';-)',';=)',':=o',':=O',':-O',\n",
    "               ';-(',';=(',':=|',':=*',':=P',':=p',':-p',':-$',':=$',':^)','|-)','I-)','I=)',':-&',':=&',':-@',':=@',\n",
    "               'x=(',':=S',':=s','8-|','8=|','B=|',':-X',':=x',':=X',':=#',':=?','(y)','(Y)','(n)','(N)']\n",
    "    count = 0\n",
    "    review = str(review)\n",
    "    if len(review) > 0:\n",
    "        for i in range(len(review)-1):\n",
    "            char1 = review[i] + review[i+1]\n",
    "            if char1 in smiley1:\n",
    "                count += 1\n",
    "        for i in range(len(review)-2):\n",
    "            char2 = review[i] + review[i+1] + review[i+2]\n",
    "            if char2 in smiley2:\n",
    "                count += 1\n",
    "        return math.sqrt(math.sqrt(math.sqrt(math.sqrt(count/float(len(review))))))\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1967,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "def stopWordsFraction(review):\n",
    "    stop_words = stopwords.words('english')\n",
    "    review = str(review)\n",
    "    count = 0\n",
    "    words  = review.split()\n",
    "    for word in words:\n",
    "        if word in stop_words:\n",
    "            count += 1\n",
    "    if len(words) > 0:\n",
    "        return count / float(len(words))\n",
    "    else:\n",
    "        return 1.0\n",
    "    \n",
    "def countPosAndNeg(x):\n",
    "    words = x.strip().split(\" \")\n",
    "    l = len(words)\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    for word in words:\n",
    "        if word in positive:\n",
    "            pos += 1\n",
    "        if word in negative:\n",
    "            neg += 1\n",
    "    return [pos*1.0/l, neg*1.0/l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
